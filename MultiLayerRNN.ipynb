{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPS/j8TnhiUsuojTaIJXANg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/learnerwcl/colab/blob/main/MultiLayerRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xl6-7YYErCl",
        "outputId": "cb5d26c7-b522-4d43-c99f-023df7300d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-23 03:38:31--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  6.37MB/s    in 46s     \n",
            "\n",
            "2025-01-23 03:39:18 (1.74 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!cd /content\n",
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -zxvf aclImdb_v1.tar.gz 2>&1 > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import roc_auc_score  # 计算 AUC\n",
        "\n",
        "from tqdm import tqdm  # 可选，用于显示进度条\n",
        "\n",
        "def grad_clipping(net, theta):\n",
        "    if isinstance(net, nn.Module):\n",
        "        params = [p for p in net.parameters() if p.requires_grad]\n",
        "    else:\n",
        "        params = net.params\n",
        "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "    if norm > theta:\n",
        "        for param in params:\n",
        "            param.grad[:] *= theta / norm\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "def build_movie_vocab_chuncked(root_dir, min_freq=20):\n",
        "    counter = Counter()\n",
        "    all_file = glob.glob(os.path.join(root_dir,\"**/*.txt\"), recursive=True)\n",
        "    for fn in all_file:\n",
        "        with open(fn, 'r') as file:\n",
        "            text = file.read()\n",
        "            text = clean_text(text)\n",
        "            words = text.split(\" \")\n",
        "            counter.update(words)\n",
        "\n",
        "    counter = {word:freq for word,freq in counter.items() if freq>=min_freq}\n",
        "    vocab = {word: idx for idx, (word, freq) in enumerate(counter.items(), start=2)}\n",
        "    vocab[\"<PAD>\"] = 0\n",
        "    vocab[\"<UNK>\"] = 1\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "oBDNg-_KE19r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "def build_movie_vocab_chuncked(root_dir, min_freq=20):\n",
        "    counter = Counter()\n",
        "    all_file = glob.glob(os.path.join(root_dir,\"**/*.txt\"), recursive=True)\n",
        "    for fn in all_file:\n",
        "        with open(fn, 'r') as file:\n",
        "            text = file.read()\n",
        "            text = clean_text(text)\n",
        "            words = text.split(\" \")\n",
        "            counter.update(words)\n",
        "\n",
        "    counter = {word:freq for word,freq in counter.items() if freq>=min_freq}\n",
        "    vocab = {word: idx for idx, (word, freq) in enumerate(counter.items(), start=2)}\n",
        "    vocab[\"<PAD>\"] = 0\n",
        "    vocab[\"<UNK>\"] = 1\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "ZBmnfpX5E4O9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_new(model, dataloader, evalloader, criterion, optimizer, device, scheduler=None, epochs=10):\n",
        "    \"\"\"\n",
        "    训练模型的通用函数。\n",
        "\n",
        "    参数：\n",
        "    - model: 定义好的神经网络模型。\n",
        "    - dataloader: 数据加载器（训练集）。\n",
        "    - criterion: 损失函数。\n",
        "    - optimizer: 优化器。\n",
        "    - device: 训练设备（\"cuda\" 或 \"cpu\"）。\n",
        "    - epochs: 训练轮数。\n",
        "\n",
        "    返回：\n",
        "    - model: 训练后的模型。\n",
        "    - metrics: 包含训练过程中的损失和其他指标。\n",
        "    \"\"\"\n",
        "    model.to(device)  # 将模型加载到设备\n",
        "    metrics = {\"loss\": [], \"auc\": [], 'eval_loss': [], 'eval_auc': []}  # 记录每个 epoch 的损失\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # 设置模型为训练模式\n",
        "        if scheduler:\n",
        "          scheduler.step()\n",
        "        epoch_loss = 0.0\n",
        "        all_labels = []  # 存储真实标签\n",
        "        all_probs = []  # 存储预测概率\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            inputs, labels = batch\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # 前向传播\n",
        "            outputs, _  = model(inputs)\n",
        "\n",
        "            # outputs = outputs.squeeze(-1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # 反向传播\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            grad_clipping(model, 1)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # 累加损失\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy()  # 假设二分类，取第二类概率\n",
        "            all_probs.extend(probs)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "        # 记录每个 epoch 的平均损失\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        metrics[\"loss\"].append(avg_loss)\n",
        "        epoch_auc = roc_auc_score(all_labels, all_probs)\n",
        "        metrics[\"auc\"].append(epoch_auc)\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss = 0.0\n",
        "        eval_labels = []  # 存储真实标签\n",
        "        eval_probs = []  # 存储预测概率\n",
        "\n",
        "        for batch_eval in evalloader:\n",
        "          inputs_eval, labels_eval = batch_eval\n",
        "          inputs_eval, labels_eval = inputs_eval.to(device), labels_eval.to(device)\n",
        "          outputs_eval, _  = model(inputs_eval)\n",
        "          loss_eval = criterion(outputs_eval, labels_eval)\n",
        "          eval_loss += loss_eval.item()\n",
        "          probs = torch.softmax(outputs_eval, dim=1)[:,1].detach().cpu().numpy()\n",
        "          eval_probs.extend(probs)\n",
        "          eval_labels.extend(labels_eval.cpu().numpy())\n",
        "        eval_loss_avg = eval_loss / len(evalloader)\n",
        "        metrics['eval_loss'].append(eval_loss_avg)\n",
        "        eval_auc = roc_auc_score(eval_labels, eval_probs)\n",
        "        metrics['eval_auc'].append(eval_auc)\n",
        "#\n",
        "\n",
        "        # print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, AUC: {epoch_auc:.4f}\")\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, AUC: {epoch_auc:.4f}, Eval Loss: {eval_loss_avg:.4f}, Eval AUC: {eval_auc:.4f}\")\n",
        "\n",
        "    return model, metrics"
      ],
      "metadata": {
        "id": "48l7DHEtGvXm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LazyLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "class ImbdDataSet(Dataset):\n",
        "  def __init__(self, root_path, vocab, max_length=128, data_type='trian', transform=None):\n",
        "    self.data_path_list = []\n",
        "    self.label_list = []\n",
        "    self.transform = transform\n",
        "    self.vocab = vocab\n",
        "    self.max_length = max_length\n",
        "\n",
        "    self.root_path = root_path\n",
        "\n",
        "    pos_path = os.path.join(root_path, data_type, 'pos')\n",
        "    neg_path = os.path.join(root_path, data_type, 'neg')\n",
        "\n",
        "    for item in glob.glob(os.path.join(pos_path,\"*.txt\")):\n",
        "      self.label_list.append(1)\n",
        "      self.data_path_list.append(item)\n",
        "\n",
        "    for item in glob.glob(os.path.join(neg_path,\"*.txt\")):\n",
        "      self.label_list.append(0)\n",
        "      self.data_path_list.append(item)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_path_list)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    label_ = self.label_list[idx]\n",
        "    path_ = self.data_path_list[idx]\n",
        "    with open(path_,'r') as f:\n",
        "      data_ = f.read()\n",
        "\n",
        "    data_ = clean_text(data_)\n",
        "\n",
        "    words = data_.split(\" \")\n",
        "    data_ = [self.vocab.get(word, self.vocab['<UNK>']) for word in words]\n",
        "\n",
        "    # 将数据处理为定长\n",
        "    if len(data_) > self.max_length:  # 截断\n",
        "        data_ = data_[:self.max_length]\n",
        "    else:  # 填充\n",
        "        data_ = data_ + [self.vocab['<PAD>']] * (self.max_length - len(data_))\n",
        "\n",
        "    if self.transform:\n",
        "        data_ = self.transform(data_)\n",
        "\n",
        "    return torch.tensor(data_, dtype=torch.long), torch.tensor(label_, dtype=torch.long)\n",
        "\n"
      ],
      "metadata": {
        "id": "S7tPpiviE6Ar"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_movie_vocab_chuncked(\"/content/aclImdb\", 256)\n",
        "print(f\"vocab size: {len(vocab)}\")\n",
        "train_data = ImbdDataSet(\"/content/aclImdb\", vocab, max_length=256, data_type='train')\n",
        "test_data = ImbdDataSet(\"/content/aclImdb\", vocab, max_length=256, data_type='test')\n",
        "train_loader = DataLoader(train_data, batch_size=2048, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=2048, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWVldbf2E7yb",
        "outputId": "cdb5e00e-8a24-4378-8c0f-c0c61c614818"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 5817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleRNNLayer(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(SimpleRNNLayer, self).__init__()\n",
        "\n",
        "    self.Wxh = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "    self.Whh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.bh = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    nn.init.xavier_uniform_(self.Wxh)\n",
        "    nn.init.xavier_uniform_(self.Whh)\n",
        "    nn.init.zeros_(self.bh)\n",
        "\n",
        "  def forward(self, inputs, h_):\n",
        "    # inputs: (batch_size, input_size)\n",
        "\n",
        "    h_ = torch.tanh(\n",
        "        inputs @ self.Wxh.T +\n",
        "        h_ @ self.Whh.T  +\n",
        "        self.bh.T\n",
        "    )\n",
        "\n",
        "    # outputs: (batch_size, hidden_size)\n",
        "\n",
        "    return h_"
      ],
      "metadata": {
        "id": "nAP1-74LFBnm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleGRULayer(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(SimpleGRULayer, self).__init__()\n",
        "\n",
        "    self.Wxr = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "    self.Whr = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.br = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
        "\n",
        "    self.Wxz = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "    self.Whz = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.bz = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
        "\n",
        "    self.Wxh = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "    self.Whh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.bh = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    nn.init.xavier_uniform_(self.Wxr)\n",
        "    nn.init.xavier_uniform_(self.Whr)\n",
        "    nn.init.zeros_(self.br)\n",
        "\n",
        "    nn.init.xavier_uniform_(self.Wxz)\n",
        "    nn.init.xavier_uniform_(self.Whz)\n",
        "    nn.init.zeros_(self.bz)\n",
        "\n",
        "    nn.init.xavier_uniform_(self.Wxh)\n",
        "    nn.init.xavier_uniform_(self.Whh)\n",
        "    nn.init.zeros_(self.bh)\n",
        "\n",
        "  def forward(self, inputs, h_):\n",
        "    # inputs: (batch_size, input_size)\n",
        "\n",
        "    r_ = torch.sigmoid(\n",
        "       inputs @ self.Wxr.T +\n",
        "       h_ @ self.Whr.T +\n",
        "       self.br.T\n",
        "    )\n",
        "\n",
        "    z_ = torch.sigmoid(\n",
        "       inputs @ self.Wxz.T +\n",
        "       h_ @ self.Whz.T +\n",
        "       self.bz.T\n",
        "    )\n",
        "\n",
        "    h_hat = torch.tanh(\n",
        "        inputs @ self.Wxh.T +\n",
        "        (r_ * h_) @ self.Whh.T  +\n",
        "        self.bh.T\n",
        "    )\n",
        "\n",
        "    h_ = z_ * h_ + (1 - z_) * h_hat\n",
        "\n",
        "    # outputs: (batch_size, hidden_size)\n",
        "\n",
        "    return h_"
      ],
      "metadata": {
        "id": "Jx6IZwMVFGDW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLSTMLayer(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(SimpleLSTMLayer, self).__init__()\n",
        "\n",
        "    self.Wxi = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "    self.Whi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.bi = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
        "\n",
        "    self.Wxf = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "    self.Whf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.bf = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
        "\n",
        "    self.Wxo = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "    self.Who = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.bo = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
        "\n",
        "    self.Wxc = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
        "    self.Whc = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "    self.bc = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
        "\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    nn.init.xavier_uniform_(self.Wxi)\n",
        "    nn.init.xavier_uniform_(self.Whi)\n",
        "    nn.init.zeros_(self.bi)\n",
        "\n",
        "    nn.init.xavier_uniform_(self.Wxf)\n",
        "    nn.init.xavier_uniform_(self.Whf)\n",
        "    nn.init.zeros_(self.bf)\n",
        "\n",
        "    nn.init.xavier_uniform_(self.Wxo)\n",
        "    nn.init.xavier_uniform_(self.Who)\n",
        "    nn.init.zeros_(self.bo)\n",
        "\n",
        "    nn.init.xavier_uniform_(self.Wxc)\n",
        "    nn.init.xavier_uniform_(self.Whc)\n",
        "    nn.init.zeros_(self.bc)\n",
        "\n",
        "  def forward(self, inputs, h_, c_):\n",
        "    # inputs: (batch_size, input_size)\n",
        "\n",
        "    i_ = torch.sigmoid(\n",
        "       inputs @ self.Wxi.T +\n",
        "       h_ @ self.Whi.T +\n",
        "       self.bi.T\n",
        "    )\n",
        "\n",
        "    f_ = torch.sigmoid(\n",
        "       inputs @ self.Wxf.T +\n",
        "       h_ @ self.Whf.T +\n",
        "       self.bf.T\n",
        "    )\n",
        "\n",
        "    o_ = torch.sigmoid(\n",
        "       inputs @ self.Wxo.T +\n",
        "       h_ @ self.Who.T +\n",
        "       self.bo.T\n",
        "    )\n",
        "\n",
        "    c_hat = torch.tanh(\n",
        "        inputs @ self.Wxc.T +\n",
        "        h_ @ self.Whc.T +\n",
        "        self.bc.T\n",
        "    )\n",
        "\n",
        "    c_ = f_ * c_ + i_ * c_hat\n",
        "\n",
        "    h_ = o_ * torch.tanh(c_)\n",
        "\n",
        "    # outputs: (batch_size, hidden_size)\n",
        "\n",
        "    return h_, c_"
      ],
      "metadata": {
        "id": "No6upM6bFIAW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, embedding_size, num_layers=1):\n",
        "    super(MultiRNN, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embed = nn.Embedding(input_size, embedding_size)\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    if num_layers == 1:\n",
        "      self.front_rnn = SimpleRNNLayer(embedding_size, hidden_size)\n",
        "    else:\n",
        "      self.front_rnn = nn.ModuleList()\n",
        "      for idx, layer_ in enumerate(range(num_layers)):\n",
        "        if idx == 0:\n",
        "          self.front_rnn.append(SimpleRNNLayer(embedding_size, hidden_size))\n",
        "        else:\n",
        "          self.front_rnn.append(SimpleRNNLayer(hidden_size, hidden_size))\n",
        "\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    if self.num_layers == 1:\n",
        "      hidden_front = torch.zeros(inputs.shape[0], self.hidden_size, device=inputs.device)\n",
        "    else:\n",
        "      hidden_front = []\n",
        "      for _ in range(self.num_layers):\n",
        "        hidden_front.append(torch.zeros(inputs.shape[0], self.hidden_size, device=inputs.device))\n",
        "\n",
        "    # 1. embedding: (batch_size, seq_len, embed_dim)\n",
        "    inputs = self.embed(inputs)\n",
        "\n",
        "    # 2. transpose: (seq_len, batch_size, embed_dim)\n",
        "    inputs = torch.transpose(inputs, 0, 1)\n",
        "\n",
        "    front_outputs = []\n",
        "\n",
        "    # 3. x: (batch_size, embed_dim)\n",
        "    for x in inputs:\n",
        "      if self.num_layers == 1:\n",
        "        tmp_hidden = self.front_rnn(x, hidden_front)\n",
        "      else:\n",
        "        for idx, hidden_ in enumerate(hidden_front):\n",
        "          if idx == 0:\n",
        "            tmp_hidden = self.front_rnn[idx](x, hidden_)\n",
        "          else:\n",
        "            tmp_hidden = self.front_rnn[idx](tmp_hidden, hidden_)\n",
        "          hidden_front[idx] = tmp_hidden\n",
        "      front_outputs.append(tmp_hidden)\n",
        "\n",
        "    hidden = self.fc(tmp_hidden)\n",
        "\n",
        "    return hidden, front_outputs"
      ],
      "metadata": {
        "id": "Ybz47ybkF-2w"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiGRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, embedding_size, num_layers=1):\n",
        "    super(MultiGRU, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embed = nn.Embedding(input_size, embedding_size)\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    if num_layers == 1:\n",
        "      self.front_rnn = SimpleGRULayer(embedding_size, hidden_size)\n",
        "    else:\n",
        "      self.front_rnn = nn.ModuleList()\n",
        "      for idx, layer_ in enumerate(range(num_layers)):\n",
        "        if idx == 0:\n",
        "          self.front_rnn.append(SimpleGRULayer(embedding_size, hidden_size))\n",
        "        else:\n",
        "          self.front_rnn.append(SimpleGRULayer(hidden_size, hidden_size))\n",
        "\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    if self.num_layers == 1:\n",
        "      hidden_front = torch.zeros(inputs.shape[0], self.hidden_size, device=inputs.device)\n",
        "    else:\n",
        "      hidden_front = []\n",
        "      for _ in range(self.num_layers):\n",
        "        hidden_front.append(torch.zeros(inputs.shape[0], self.hidden_size, device=inputs.device))\n",
        "\n",
        "    # 1. embedding: (batch_size, seq_len, embed_dim)\n",
        "    inputs = self.embed(inputs)\n",
        "\n",
        "    # 2. transpose: (seq_len, batch_size, embed_dim)\n",
        "    inputs = torch.transpose(inputs, 0, 1)\n",
        "\n",
        "    front_outputs = []\n",
        "\n",
        "    # 3. x: (batch_size, embed_dim)\n",
        "    for x in inputs:\n",
        "      if self.num_layers == 1:\n",
        "        tmp_hidden = self.front_rnn(x, hidden_front)\n",
        "      else:\n",
        "        for idx, hidden_ in enumerate(hidden_front):\n",
        "          if idx == 0:\n",
        "            tmp_hidden = self.front_rnn[idx](x, hidden_)\n",
        "          else:\n",
        "            tmp_hidden = self.front_rnn[idx](tmp_hidden, hidden_)\n",
        "          hidden_front[idx] = tmp_hidden\n",
        "      front_outputs.append(tmp_hidden)\n",
        "\n",
        "    hidden = self.fc(tmp_hidden)\n",
        "\n",
        "    return hidden, front_outputs"
      ],
      "metadata": {
        "id": "x2edJmZrN2YS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, embedding_size, num_layers=1):\n",
        "    super(MultiLSTM, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embed = nn.Embedding(input_size, embedding_size)\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    if num_layers == 1:\n",
        "      self.front_rnn = SimpleLSTMLayer(embedding_size, hidden_size)\n",
        "    else:\n",
        "      self.front_rnn = nn.ModuleList()\n",
        "      for idx, layer_ in enumerate(range(num_layers)):\n",
        "        if idx == 0:\n",
        "          self.front_rnn.append(SimpleLSTMLayer(embedding_size, hidden_size))\n",
        "        else:\n",
        "          self.front_rnn.append(SimpleLSTMLayer(hidden_size, hidden_size))\n",
        "\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    if self.num_layers == 1:\n",
        "      hidden_front = torch.zeros(inputs.shape[0], self.hidden_size, device=inputs.device)\n",
        "      hidden_cell = torch.zeros(inputs.shape[0], self.hidden_size, device=inputs.device)\n",
        "    else:\n",
        "      hidden_front = []\n",
        "      hidden_cell = []\n",
        "      for _ in range(self.num_layers):\n",
        "        hidden_front.append(torch.zeros(inputs.shape[0], self.hidden_size, device=inputs.device))\n",
        "        hidden_cell.append(torch.zeros(inputs.shape[0], self.hidden_size, device=inputs.device))\n",
        "\n",
        "    # 1. embedding: (batch_size, seq_len, embed_dim)\n",
        "    inputs = self.embed(inputs)\n",
        "\n",
        "    # 2. transpose: (seq_len, batch_size, embed_dim)\n",
        "    inputs = torch.transpose(inputs, 0, 1)\n",
        "\n",
        "    front_outputs = []\n",
        "\n",
        "    # 3. x: (batch_size, embed_dim)\n",
        "    for x in inputs:\n",
        "      if self.num_layers == 1:\n",
        "        tmp_hidden, tmp_cell = self.front_rnn(x, hidden_front, hidden_cell)\n",
        "      else:\n",
        "        for idx, (hidden_, cell_) in enumerate(zip(hidden_front, hidden_cell)):\n",
        "          if idx == 0:\n",
        "            tmp_hidden, tmp_cell = self.front_rnn[idx](x, hidden_, cell_)\n",
        "          else:\n",
        "            tmp_hidden, tmp_cell = self.front_rnn[idx](tmp_hidden, hidden_, cell_)\n",
        "          hidden_front[idx] = tmp_hidden\n",
        "          hidden_cell[idx] = tmp_cell\n",
        "      front_outputs.append(tmp_hidden)\n",
        "\n",
        "    hidden = self.fc(tmp_hidden)\n",
        "\n",
        "    return hidden, front_outputs"
      ],
      "metadata": {
        "id": "WtgsxTT7OP-t"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiRNN(len(vocab), 128, 2, 128, num_layers=3)  # 定义的网络结构\n",
        "criterion = torch.nn.CrossEntropyLoss()  # 损失函数\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # 优化器\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 设置设备\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoPk3QwqGozH",
        "outputId": "dcb65d5d-e337-41f0-c92c-e9fdcc40d6b4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, metrics = train_model_new(\n",
        "    model=model,\n",
        "    dataloader=train_loader,\n",
        "    evalloader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_AN8lf5G2wl",
        "outputId": "b0a2cfc7-2305-4ca2-e6ca-30155a983747"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 13/13 [00:07<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7101, AUC: 0.5016, Eval Loss: 0.7071, Eval AUC: 0.5044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 13/13 [00:07<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.6885, AUC: 0.5496, Eval Loss: 0.6961, Eval AUC: 0.5083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 13/13 [00:07<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.6778, AUC: 0.5843, Eval Loss: 0.6973, Eval AUC: 0.4993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 13/13 [00:07<00:00,  1.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 0.6677, AUC: 0.6108, Eval Loss: 0.7013, Eval AUC: 0.5067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 0.6558, AUC: 0.6254, Eval Loss: 0.7078, Eval AUC: 0.5082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 0.6419, AUC: 0.6458, Eval Loss: 0.7166, Eval AUC: 0.5086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 0.6175, AUC: 0.6753, Eval Loss: 0.7329, Eval AUC: 0.5053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 13/13 [00:07<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 0.5891, AUC: 0.6938, Eval Loss: 0.7628, Eval AUC: 0.5046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 13/13 [00:07<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 0.5583, AUC: 0.7245, Eval Loss: 0.8048, Eval AUC: 0.5071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 13/13 [00:07<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 0.5298, AUC: 0.7373, Eval Loss: 0.8404, Eval AUC: 0.5082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiGRU(len(vocab), 128, 2, 128, num_layers=3)  # 定义的网络结构\n",
        "criterion = torch.nn.CrossEntropyLoss()  # 损失函数\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # 优化器\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 设置设备\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7iCWOZ_G7GP",
        "outputId": "5f88dadd-34b8-4cc1-ef90-d82b558df97c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, metrics = train_model_new(\n",
        "    model=model,\n",
        "    dataloader=train_loader,\n",
        "    evalloader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUpYWCvvOFgI",
        "outputId": "424ed71a-d247-45ab-9a85-8cade948aacc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 13/13 [00:16<00:00,  1.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7141, AUC: 0.5038, Eval Loss: 0.6926, Eval AUC: 0.5104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 13/13 [00:17<00:00,  1.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.6901, AUC: 0.5370, Eval Loss: 0.6933, Eval AUC: 0.5254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 13/13 [00:16<00:00,  1.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.6845, AUC: 0.5621, Eval Loss: 0.6933, Eval AUC: 0.5403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 13/13 [00:17<00:00,  1.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 0.6777, AUC: 0.5779, Eval Loss: 0.6926, Eval AUC: 0.5563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 13/13 [00:16<00:00,  1.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 0.6629, AUC: 0.6232, Eval Loss: 0.6846, Eval AUC: 0.6235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 13/13 [00:17<00:00,  1.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 0.6147, AUC: 0.7220, Eval Loss: 0.6556, Eval AUC: 0.7010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 13/13 [00:17<00:00,  1.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 0.5495, AUC: 0.7941, Eval Loss: 0.5273, Eval AUC: 0.8184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 13/13 [00:16<00:00,  1.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 0.5182, AUC: 0.8289, Eval Loss: 0.6148, Eval AUC: 0.7951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 13/13 [00:16<00:00,  1.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 0.4686, AUC: 0.8604, Eval Loss: 0.5059, Eval AUC: 0.8384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 13/13 [00:16<00:00,  1.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 0.3952, AUC: 0.9043, Eval Loss: 0.4822, Eval AUC: 0.8605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiLSTM(len(vocab), 128, 2, 128, num_layers=3)  # 定义的网络结构\n",
        "criterion = torch.nn.CrossEntropyLoss()  # 损失函数\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # 优化器\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 设置设备\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ASxx8DYOJFr",
        "outputId": "e69a5236-d55e-484e-c11e-ab3859e87e80"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, metrics = train_model_new(\n",
        "    model=model,\n",
        "    dataloader=train_loader,\n",
        "    evalloader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPFdDbT7PJ43",
        "outputId": "139498fe-2dab-48b0-ea08-f9d85f82de76"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 13/13 [00:19<00:00,  1.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6936, AUC: 0.5064, Eval Loss: 0.6943, Eval AUC: 0.4981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 13/13 [00:20<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.6913, AUC: 0.5228, Eval Loss: 0.6920, Eval AUC: 0.5252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 13/13 [00:19<00:00,  1.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.6882, AUC: 0.5455, Eval Loss: 0.6871, Eval AUC: 0.5545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 13/13 [00:20<00:00,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 0.6777, AUC: 0.5739, Eval Loss: 0.6926, Eval AUC: 0.5771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 13/13 [00:20<00:00,  1.56s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 0.6671, AUC: 0.6024, Eval Loss: 0.6707, Eval AUC: 0.6036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 13/13 [00:20<00:00,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 0.6577, AUC: 0.6175, Eval Loss: 0.6748, Eval AUC: 0.6017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 13/13 [00:20<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 0.6458, AUC: 0.6408, Eval Loss: 0.6728, Eval AUC: 0.6538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 13/13 [00:20<00:00,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 0.6301, AUC: 0.6588, Eval Loss: 0.6720, Eval AUC: 0.7202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 13/13 [00:20<00:00,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 0.6129, AUC: 0.6848, Eval Loss: 0.6624, Eval AUC: 0.6572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 13/13 [00:20<00:00,  1.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 0.5697, AUC: 0.7878, Eval Loss: 0.5846, Eval AUC: 0.7489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTrxqkoQPPTO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}